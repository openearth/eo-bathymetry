{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import dask\n",
    "import xarray as xr\n",
    "import pyproj\n",
    "import rioxarray\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "import rasterio\n",
    "from rasterio.features import shapes\n",
    "from shapely.geometry import shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign_projection(ds, epsg=None):\n",
    "\n",
    "    if not epsg == None:\n",
    "        proj = pyproj.CRS.from_epsg(int(epsg))\n",
    "    else:\n",
    "        proj = pyproj.CRS.from_epsg(int(ds.crs.values.tolist()))\n",
    "\n",
    "    print(proj,\"projection was assigned to the dataset attributes\")\n",
    "    ds.attrs['crs'] = proj\n",
    "    return ds\n",
    "\n",
    "def print_ds_properties(rds,epsg=None):\n",
    "    # Print the grid size\n",
    "    print(\"Grid size:\", rds.rio.resolution())\n",
    "\n",
    "    #Print null data\n",
    "    print(\"no data:\", rds.rio.nodata)\n",
    "\n",
    "    # Print the projection information\n",
    "    if rds.rio.crs == None:\n",
    "        print(\"There is no projection\")\n",
    "        proj = pyproj.CRS.from_epsg(epsg)\n",
    "        rds.attrs['crs'] = proj\n",
    "        rds.rio.set_crs(proj, inplace=True)\n",
    "    else:\n",
    "        print(\"There is projection available\")\n",
    "    \n",
    "    print(\"Projection EPSG code is:\", rds.rio.crs, \"\\n\")\n",
    "    return\n",
    "\n",
    "def change_resolution(ds, new_resolution):\n",
    "    # Reproject the rioxarray object to the new resolution\n",
    "    reprojected_ds = ds.rio.reproject(ds.rio.crs, resolution=new_resolution, resampling=\"bilinear\")\n",
    "    return reprojected_ds\n",
    "\n",
    "def match_resolution(rds, rds_source):\n",
    "    # Reproject the rioxarray object to the new resolution\n",
    "    reprojected_ds = rds.rio.reproject(rds_source.rio.crs, resolution=rds_source.rio.resolution(), resampling= rioxarray.enums.Resampling.bilinear)\n",
    "    return reprojected_ds\n",
    "\n",
    "def redefine_null_for_nan(ds, new_null_value):\n",
    "    # Replace NaN values with the new null value\n",
    "    ds.values[np.isnan(ds.values)] = new_null_value\n",
    "    # ds.rio.update({'nodata': new_null_value})\n",
    "    return ds\n",
    "\n",
    "def create_gdf_from_geojson_files(input_aoi_data):\n",
    "    geojson_files = []\n",
    "\n",
    "    for filename in os.listdir(input_aoi_data):\n",
    "        if filename.endswith(\".geojson\"):\n",
    "            gdf = gpd.read_file(os.path.join(input_aoi_data,filename))\n",
    "            aoi_id = filename.split('_')\n",
    "            aoi_id = aoi_id[-1].split('.')[0]\n",
    "            gdf.insert(1, \"aoi\", aoi_id)\n",
    "            geojson_files.append((gdf))\n",
    "\n",
    "    aoi_gdf = gpd.GeoDataFrame(pd.concat(geojson_files, ignore_index=True)).drop(columns=[\"id\"])\n",
    "    return aoi_gdf\n",
    "\n",
    "def clip_raster(rds, geometry):\n",
    "    rds_clipped = rds.rio.clip(geometry)\n",
    "    return rds_clipped\n",
    "\n",
    "def clip_raster_with_gdf(rds, gdf):\n",
    "    rds_clipped_list = []\n",
    "    for index, row in gdf.iterrows():\n",
    "        aoi = row[\"aoi\"]\n",
    "        try:\n",
    "            geometry = gdf.iloc[index:index+1].geometry\n",
    "            rds_clipped = rds.rio.clip(geometry)\n",
    "            rds_clipped_list.append(rds_clipped)\n",
    "            print(f\"Successful processing row {index} {aoi}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing row {index} {aoi}: {e}\")\n",
    "            rds_clipped_list.append(\"NaN\")\n",
    "            continue\n",
    "    print(\"\\n\")\n",
    "    return rds_clipped_list \n",
    "\n",
    "def apply_lat_mask(raster1, raster2):\n",
    "    # Read the raster data\n",
    "    data1 = raster1.values\n",
    "    data2 = raster2.values\n",
    "    \n",
    "    # Create a new array for the result raster\n",
    "    result_data = np.full_like(data1, fill_value=np.nan, dtype='float32')\n",
    "    \n",
    "    # Compare pixel values and assign new values\n",
    "    result_data[(data1 >= data2)] = 7\n",
    "    result_data[(data1 < data2)] = 9\n",
    "    \n",
    "    # Create a new rioxarray dataset for the result raster\n",
    "    result_raster = raster1.copy(data=result_data)\n",
    "    \n",
    "    return result_raster\n",
    "\n",
    "def apply_gebco_mask(raster1, raster2):\n",
    "    # Read the raster data\n",
    "    data1 = raster1.values\n",
    "    data2 = raster2.values\n",
    "    \n",
    "    # Create a new array for the result raster\n",
    "    result_data = np.full_like(data1, fill_value=np.nan, dtype='float32')\n",
    "    \n",
    "    # Compare pixel values and assign new values\n",
    "    result_data[(data2 > 10)] = 11\n",
    "    \n",
    "    # Create a new rioxarray dataset for the result raster\n",
    "    result_raster = raster1.copy(data=result_data)\n",
    "    \n",
    "    return result_raster\n",
    "\n",
    "def apply_lat_hat_mask(depth, lat, hat):\n",
    "    # Read the raster data\n",
    "    depth_array = depth.values\n",
    "    lat_array = lat.values\n",
    "    hat_array = hat.values\n",
    "    \n",
    "    # Create a new array for the result raster\n",
    "    result_data = np.full_like(depth_array, fill_value=np.nan, dtype='float32')\n",
    "    \n",
    "    # Compare pixel values and assign new values\n",
    "\n",
    "    # result_data[( hat_array >= depth_array) ] = 7 \n",
    "    # result_data[(hat_array >= depth_array) & (depth_array >= lat_array)] = 7\n",
    "    # result_data[(hat_array < depth_array)] = 11\n",
    "    # result_data[(depth_array < lat_array)] = 9\n",
    "    # result_data[(depth_array >= lat_array) & (hat_array >= depth_array)] = 7\n",
    "    result_data[(hat_array >= depth_array)] = 7\n",
    "    result_data[(hat_array < depth_array)] = 9\n",
    "    \n",
    "    # Create a new rioxarray dataset for the result raster\n",
    "    result_raster = depth.copy(data=result_data)\n",
    "    \n",
    "    return result_raster\n",
    "\n",
    "def apply_gebco_mask(gebco_rds,lat_mask):\n",
    "\n",
    "    binary_mask = gebco_rds > 10\n",
    "    lat_mask_gebco_plus_10 = lat_mask.where(~binary_mask, other=11)\n",
    "\n",
    "    return lat_mask_gebco_plus_10 \n",
    "\n",
    "def apply_lat_mask(depth, lat):\n",
    "\n",
    "    binary_mask = lat > depth\n",
    "    depth_lat = depth.where(~binary_mask, other=np.nan)\n",
    "    return depth_lat \n",
    "\n",
    "def apply_hat_mask(depth_lat, hat_rds):\n",
    "\n",
    "    binary_mask =  depth_lat > hat_rds \n",
    "    depth_lat_hat = depth_lat.where(~binary_mask, other=np.nan)\n",
    "    return depth_lat_hat\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define packages paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data path already exists: d:\\Proyectos2024\\Copernicus\\Repository\\Copernicus\\Repository\\eo-bathymetry\\notebooks\\Output\n"
     ]
    }
   ],
   "source": [
    "repository_path = os.path.dirname(os.getcwd())\n",
    "input_data_path = os.path.join(repository_path,\"Data\")\n",
    "input_aoi_data = r\"p:\\11209821-cmems-global-sdb\\00_miscellaneous\\AOIs\"\n",
    "output_data_path = os.path.join(repository_path,\"Output\")\n",
    "\n",
    "if not os.path.exists(output_data_path):\n",
    "    print(\"Output data path does not exist. Creating directory...\")\n",
    "    os.makedirs(output_data_path)\n",
    "    print(\"Output data path created:\", output_data_path)\n",
    "else:\n",
    "    print(\"Input data path already exists:\", output_data_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieve input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AOIs for al the sites\n",
    "aoi_gdf = create_gdf_from_geojson_files(input_aoi_data)\n",
    "aoi_gdf_bounding_box  = aoi_gdf.copy()\n",
    "aoi_gdf_bounding_box['bounding_box'] = aoi_gdf_bounding_box.geometry.apply(lambda x: x.envelope)\n",
    "aoi_gdf_bounding_box = aoi_gdf_bounding_box[['aoi', 'bounding_box']].rename(columns={'bounding_box': 'geometry'})\n",
    "\n",
    "# AOI for Sao Paulo\n",
    "aoi_saopaulo_bounding_box = aoi_gdf_bounding_box[aoi_gdf_bounding_box['aoi'] == \"SaoPaulo\"]\n",
    "aoi_saopaulo_bounding_box.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# AOI for WaddenSea\n",
    "aoi_wadden_bounding_box = aoi_gdf_bounding_box[aoi_gdf_bounding_box['aoi'] == \"WaddenSea\"]\n",
    "aoi_wadden_bounding_box.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# AOI for Okha\n",
    "aoi_woody_bounding_box = aoi_gdf_bounding_box[aoi_gdf_bounding_box['aoi'] == \"WoodyCape\"]\n",
    "aoi_woody_bounding_box.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# aoi_saopaulo_bounding_box.to_file(os.path.join(output_data_path,'aoi_saopaulo_bounding_box.geojson'), driver='GeoJSON')\n",
    "# aoi_wadden_bounding_box.to_file(os.path.join(output_data_path,'aoi_wadden_bounding_box.geojson'), driver='GeoJSON')\n",
    "# aoi_woody_bounding_box.to_file(os.path.join(output_data_path,'aoi_woody_bounding_box.geojson'), driver='GeoJSON')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grid size: (0.004166666666666666, -0.004166666666666667)\n",
      "no data: None\n",
      "There is no projection\n",
      "Projection EPSG code is: EPSG:4326 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "output_data_path = os.path.join(repository_path,\"AOI_results\")\n",
    "coordinates_list = ['n90.0_s0.0_w0.0_e90.0']\n",
    "# coordinates_list = ['n0.0_s-90.0_w-90.0_e0.0']\n",
    "# coordinates_list = ['n0.0_s-90.0_w0.0_e90.0'] \n",
    "\n",
    "coordinates = coordinates_list[0]\n",
    "results = rioxarray.open_rasterio(os.path.join(output_data_path,f\"gebco_2023_{coordinates}_result.tif\"))\n",
    "print_ds_properties(results, 4326)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clip results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successful processing row 0 WoodyCape\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# results_clipped_wadden = clip_raster_with_gdf(results, aoi_wadden_bounding_box)\n",
    "# results_clipped_wadden[0].rio.to_raster(os.path.join(output_data_path,f'gebco_2023_{coordinates}_results_clipped_wadden.tif'), driver='GTiff', compress='lzw')\n",
    "\n",
    "# results_clipped_saopaulo = clip_raster_with_gdf(results, aoi_saopaulo_bounding_box)\n",
    "# results_clipped_saopaulo[0].rio.to_raster(os.path.join(output_data_path,f'gebco_2023_{coordinates}_results_clipped_saopaulo.tif'), driver='GTiff', compress='lzw')\n",
    "\n",
    "# results_clipped_woody = clip_raster_with_gdf(results, aoi_woody_bounding_box)\n",
    "# results_clipped_woody[0].rio.to_raster(os.path.join(output_data_path,f'gebco_2023_{coordinates}_results_clipped_woody.tif'), driver='GTiff', compress='lzw')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open raster file using rasterio\n",
    "with rasterio.open(os.path.join(output_data_path,f\"gebco_2023_{coordinates}_results_clipped_wadden.tif\")) as src:\n",
    "    # Read raster data into numpy array\n",
    "    raster_array = src.read(1)  # Assuming it's a single band raster, adjust if necessary\n",
    "    # Extract transformation metadata\n",
    "    transform = src.transform\n",
    "    # Polygonize raster data\n",
    "    polygons = list(shapes(raster_array, mask=None, transform=transform))\n",
    "    # Convert polygons to Shapely geometries and record pixel values\n",
    "    geometries_with_values = [(shape(polygon), value) for polygon, value in polygons]\n",
    "\n",
    "# Extract geometries and values into separate lists\n",
    "geometries = [geometry for geometry, value in geometries_with_values]\n",
    "values = [value for geometry, value in geometries_with_values]\n",
    "\n",
    "# Convert Shapely geometries and pixel values to GeoDataFrame\n",
    "geo_df = gpd.GeoDataFrame(geometry=geometries, data={'pixel_value': values})\n",
    "\n",
    "geo_df = geo_df[~np.isnan(geo_df['pixel_value'])]\n",
    "geo_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Define the EPSG code for the desired projection\n",
    "epsg_code = 4326  # For example, EPSG code for WGS 84\n",
    "\n",
    "# Assign the projection to the GeoDataFrame\n",
    "geo_df.crs = f\"EPSG:{epsg_code}\"\n",
    "\n",
    "# Save GeoDataFrame to file\n",
    "# geo_df.to_file(os.path.join(output_data_path,f\"gebco_2023_{coordinates}_depth_lat_hat.shp\"))\n",
    "geo_df.to_file(os.path.join(output_data_path,f\"gebco_2023_{coordinates}_results_clipped_wadden.geojson\"), driver=\"GeoJSON\", crs=f\"EPSG:{epsg_code}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "copernicus",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
