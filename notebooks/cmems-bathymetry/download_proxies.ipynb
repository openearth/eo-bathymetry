{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kras\\AppData\\Local\\Temp\\ipykernel_8748\\3728536483.py:9: UserWarning: Shapely 2.0 is installed, but because PyGEOS is also installed, GeoPandas will still use PyGEOS by default for now. To force to use and test Shapely 2.0, you have to set the environment variable USE_PYGEOS=0. You can do this before starting the Python process, or in your code before importing geopandas:\n",
      "\n",
      "import os\n",
      "os.environ['USE_PYGEOS'] = '0'\n",
      "import geopandas\n",
      "\n",
      "In a future release, GeoPandas will switch to using Shapely by default. If you are using PyGEOS directly (calling PyGEOS functions on geometries from GeoPandas), this will then stop working and you are encouraged to migrate from PyGEOS to Shapely 2.0 (https://shapely.readthedocs.io/en/latest/migration_pygeos.html).\n",
      "  import geopandas as gpd\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "@author: Etienne Kras\n",
    "\"\"\"\n",
    "\n",
    "# generic imports\n",
    "import sys\n",
    "import os\n",
    "import geopandas as gpd\n",
    "import time\n",
    "import geemap\n",
    "import geojson\n",
    "import ee\n",
    "ee.Initialize()\n",
    "\n",
    "# specific imports\n",
    "from typing import Any, Dict, List, Optional\n",
    "from geojson import Polygon, Feature, FeatureCollection, dump\n",
    "from shapely.geometry import Polygon\n",
    "from dateutil.relativedelta import *\n",
    "from google.cloud import storage\n",
    "from logging import Logger, getLogger\n",
    "from googleapiclient.discovery import build\n",
    "from re import sub\n",
    "from ctypes import ArgumentError\n",
    "from functools import partial\n",
    "from dateutil.parser import parse\n",
    "\n",
    "# custom functionality import without requirement to pip install package\n",
    "local_path = r\"C:\\Users\\kras\\Documents\\GitHub\\ee-packages-py\"  # path to local GitHub clone\n",
    "sys.path.append(local_path)\n",
    "from eepackages.applications.bathymetry import Bathymetry\n",
    "from eepackages import tiler\n",
    "\n",
    "logger: Logger = getLogger(__name__)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project specific toggles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# acknowledgements & code references:\n",
    "# https://github.com/openearth/eo-bathymetry/\n",
    "# https://github.com/openearth/eo-bathymetry-functions/\n",
    "# https://github.com/gee-community/ee-packages-py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# see scheme at https://github.com/openearth/eo-bathymetry/blob/master/notebooks/rws-bathymetry/acces_api.pdf for a workflow visualization \n",
    "\n",
    "# project toggles\n",
    "main_fol = r\"p:/11208561-he11-abu-dhabi/6_satellite_bathymetry\" # name of the main local folder \n",
    "bucket = \"hudayriat-sdb\" # name of the Google Cloud Storage bucket to store files in the cloud\n",
    "credential_file = r\"Z:/OneDrive - Stichting Deltares/Documents/3. General/keys/jip-calm-0162576b9743.json\" # Cloud Storage credential key\n",
    "output_fol = \"Proxy\" # name of the overall project\n",
    "project_name = \"Hudayriat\" # name of the project AoI\n",
    "draw_AoI = 0 # toggle 1 to draw AoI, 0 to load\n",
    "\n",
    "# composite image toggles\n",
    "mode = \"subtidal\" # specify mode, either \"intertidal\" or \"subtidal\"\n",
    "start_date = \"2021-01-01\" # start date of the composites\n",
    "stop_date = \"2022-01-01\" # end date of the composites\n",
    "compo_int = 12 # composite interval [months]\n",
    "compo_len = 12 # composite length [months]\n",
    "scale = 10  # output resolution of the image [m]\n",
    "\n",
    "# tiling options\n",
    "zoomed_list = [9, 10, 11] # list with zoom levels to be inspected\n",
    "sel_tile = 0 # idx of chosen tile level in zoomed_list (inspect the map to chose it accordingly), z9 too big for in memory computations\n",
    "# note, see https://www.openearth.nl/rws-bathymetry/2019.html; Z9 is optimal size..\n",
    "\n",
    "# load google credentials, if specified\n",
    "if not credential_file == \"\":  \n",
    "    os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = str(credential_file)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-processing using the API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading and visualizing AoI\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55dc0a45692e4bf1985f0c7426d00529",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map(center=[24.33, 54.3], controls=(WidgetControl(options=['position', 'transparent_bg'], widget=HBox(childrenâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# draw or load Area of Interest (AoI)\n",
    "\n",
    "# TODO: take center of AOI input file if present, put in random coordinate and let user find a place and draw a polygon\n",
    "# TODO: fix horizontal tiling error (DOS) in API (to use multiple tiles) or move to single polygon run if AoI crosses multiple tiles\n",
    "Map = geemap.Map(center=(24.33, 54.30), zoom=10) # initialize map with base in Hudayriat\n",
    "\n",
    "if draw_AoI == 1:\n",
    "    print(\"Please draw a polygon somewhere in a water body\") # identifier\n",
    "if draw_AoI == 0:\n",
    "    # open AoI\n",
    "    print(\"Loading and visualizing AoI\") #identifier\n",
    "    #AoIee = geemap.geojson_to_ee(os.path.join(main_fol,'AOI',project_name+'.geojson'))\n",
    "\n",
    "    with open(os.path.join(main_fol, \"AOI\", project_name + \".geojson\"), 'r') as f:\n",
    "        contents = geojson.loads(f.read())\n",
    "    AoIee = ee.Geometry(contents[\"features\"][0][\"geometry\"])\n",
    "\n",
    "    Map.addLayer(AoIee, {}, \"AoI\")\n",
    "\n",
    "Map # show map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reconstructing AoI from loaded file\n"
     ]
    }
   ],
   "source": [
    "# (re)construct the AoI\n",
    "\n",
    "if draw_AoI == 1:\n",
    "    \n",
    "    print(\"Constructing AoI from drawn polygon\") # identifier\n",
    "    \n",
    "    # get AoI \n",
    "    AoIee = ee.FeatureCollection(Map.draw_features) # make featurecollection\n",
    "    AoI = Polygon(AoIee.getInfo()[\"features\"][0][\"geometry\"][\"coordinates\"][0]) # create AoI shapefile\n",
    "\n",
    "    # export AoI\n",
    "    features = []\n",
    "    features.append(Feature(geometry=AoI, properties={\"AoI\": project_name}))\n",
    "    feature_collection = FeatureCollection(features)\n",
    "    with open(os.path.join(main_fol,\"AOI\",project_name + \".geojson\"), \"w\") as f: # geojson\n",
    "        dump(feature_collection, f)\n",
    "    gdr = gpd.GeoDataFrame({\"properties\":{\"AoI\": project_name}, \"geometry\": AoI}, crs=\"EPSG:4326\") #shp\n",
    "    gdr.to_file(os.path.join(main_fol,\"AOI\",project_name+\".shp\"))\n",
    "    bounds = ee.Geometry.Polygon([[[a,b] for a, b in zip(*AoI.exterior.coords.xy)]])\n",
    "    \n",
    "if draw_AoI == 0:\n",
    "    print(\"Reconstructing AoI from loaded file\")\n",
    "    # get AoI\n",
    "    with open(os.path.join(main_fol,\"AOI\",project_name+\".geojson\")) as f:\n",
    "        AoIjson = geojson.load(f)\n",
    "    try: # drawn polygon in this script\n",
    "        AoI = Polygon(AoIjson[\"features\"][0][\"geometry\"][\"coordinates\"]) \n",
    "    except: # drawn in QGIS / ArcGIS and written to geojson there (client file)\n",
    "        AoI = Polygon(AoIjson[\"features\"][0][\"geometry\"][\"coordinates\"][0])\n",
    "    bounds = ee.Geometry.Polygon([[[a,b] for a, b in zip(*AoI.exterior.coords.xy)]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tiling the AoI\n",
    "def add_tile_bounds(zoom):\n",
    "    tiled = tiler.get_tiles_for_geometry(bounds, zoom)\n",
    "    Map.addLayer(tiled.style(width=max(1, 10 - zoom), fillColor= \"00000022\"), {}, \"tiles \" + str(zoom))\n",
    "\n",
    "    return(tiled)\n",
    "\n",
    "tiles = list(map(add_tile_bounds, zoomed_list)) # add tiles for different zoom levels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export selected tiles (represented by zoom level) to geojsons\n",
    "# note, adjust sel_tile accordingly \n",
    "for idx, tile in enumerate(tiles[sel_tile].getInfo()[\"features\"]):\n",
    "    tile_pol = Polygon(tile['geometry']['coordinates'][0]) # create tile polygon\n",
    "    tile_name = \"z%s_x%s_y%s\"%(tile[\"properties\"][\"zoom\"], int(float(tile[\"properties\"][\"tx\"])), int(float(tile[\"properties\"][\"ty\"])))\n",
    "\n",
    "    features = []\n",
    "    features.append(Feature(geometry=tile_pol, properties={\"name\": tile_name}))\n",
    "    feature_collection = FeatureCollection(features)\n",
    "    feature_collection.crs = {\"type\": \"name\",\"properties\": {\"name\": \"epsg:3857\"}} # default EE projection\n",
    "    with open(os.path.join(main_fol,\"AOI Polygons\",tile_name + \".geojson\"), \"w\") as f: # geojson\n",
    "        dump(feature_collection, f)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute SDB using the API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# functions to compute sub & intertidal bathymetry proxies based on standardized SlippyMap tiling practice\n",
    "# functions taken from: https://github.com/openearth/eo-bathymetry/blob/master/notebooks/rws-bathymetry/export_bathymetry.ipynb\n",
    "# resembles similar behaviour as in https://github.com/openearth/eo-bathymetry-functions but slightly adjusted for local study \n",
    "\n",
    "def get_tile_subtidal_bathymetry(tile: ee.Feature, start: ee.String, stop: ee.String) -> ee.Image:\n",
    "    \"\"\"\n",
    "    Get subtidal bathymetry based on tile geometry.\n",
    "    Server-side compliant for GEE.\n",
    "\n",
    "    args:\n",
    "        tile (ee.Feature): tile geometry used to obtain bathymetry.\n",
    "        start (ee.String): start date in YYYY-MM-dd format.\n",
    "        stop (ee.String): stop date in YYYY-MM-dd format.\n",
    "    \n",
    "    returns:\n",
    "        ee.Image: image containing subtidal bathymetry covering tile.\n",
    "    \"\"\"\n",
    "\n",
    "    bounds: ee.Geometry = ee.Feature(tile).geometry().bounds(1)\n",
    "    sdb: Bathymetry = Bathymetry()\n",
    "    zoom: ee.String = ee.String(tile.get(\"zoom\"))\n",
    "    tx: ee.String = ee.String(tile.get(\"tx\"))\n",
    "    ty: ee.String = ee.String(tile.get(\"ty\"))\n",
    "    tile_name: ee.String = ee.String(\"z\").cat(zoom).cat(\"_x\").cat(tx).cat(\"_y\").cat(ty).replace(\"\\.\\d+\", \"\", \"g\")\n",
    "    img_fullname: ee.String = ee.String(tile_name).cat(\"_t\").cat(ee.Date(start).millis().format())\n",
    "        \n",
    "    image: ee.Image = sdb.compute_inverse_depth(\n",
    "                bounds=bounds,\n",
    "                start=start,\n",
    "                stop=stop,\n",
    "                scale=tiler.zoom_to_scale(ee.Number.parse(tile.get(\"zoom\"))).multiply(5), # scale to search for clean images\n",
    "                missions=[\"S2\", \"L8\"],\n",
    "                filter_masked=True,\n",
    "                skip_neighborhood_search=False\n",
    "                # cloud_frequency_threshold_data=,\n",
    "                # pansharpen=,\n",
    "                # skip_scene_boundary_fix=,\n",
    "                # bounds_buffer=\n",
    "    ).clip(bounds)\n",
    "\n",
    "    image = image.set(\n",
    "        \"fullname\", img_fullname,\n",
    "        \"system:time_start\", ee.Date(start).millis(),\n",
    "        \"system:time_stop\", ee.Date(stop).millis(),\n",
    "        \"zoom\", zoom,\n",
    "        \"tx\", tx,\n",
    "        \"ty\", ty\n",
    "    )\n",
    "    return image\n",
    "\n",
    "def get_tile_intertidal_bathymetry(tile: ee.Feature, start: ee.String, stop: ee.String) -> ee.Image:\n",
    "    \"\"\"\n",
    "    Get intertidal bathymetry based on tile geometry.\n",
    "    Server-side compliant for GEE.\n",
    "\n",
    "    args:\n",
    "        tile (ee.Feature): tile geometry used to obtain bathymetry.\n",
    "        start (ee.String): start date in YYYY-MM-dd format.\n",
    "        stop (ee.String): stop date in YYYY-MM-dd format.\n",
    "    \n",
    "    returns:\n",
    "        ee.Image: image containing intertidal bathymetry covering tile.\n",
    "    \"\"\"\n",
    "\n",
    "    bounds: ee.Geometry = ee.Feature(tile).geometry().bounds(1)\n",
    "    sdb: Bathymetry = Bathymetry()\n",
    "    zoom: ee.String = ee.String(tile.get(\"zoom\"))\n",
    "    tx: ee.String = ee.String(tile.get(\"tx\"))\n",
    "    ty: ee.String = ee.String(tile.get(\"ty\"))\n",
    "    tile_name: ee.String = ee.String(\"z\").cat(zoom).cat(\"_x\").cat(tx).cat(\"_y\").cat(ty).replace(\"\\.\\d+\", \"\", \"g\")\n",
    "    img_fullname: ee.String = ee.String(tile_name).cat(\"_t\").cat(ee.Date(start).millis().format())\n",
    "        \n",
    "    image: ee.Image = sdb.compute_intertidal_depth(\n",
    "        bounds=bounds,\n",
    "        start=start,\n",
    "        stop=stop,\n",
    "        scale=tiler.zoom_to_scale(ee.Number.parse(tile.get(\"zoom\"))).multiply(5), # scale to search for clean images\n",
    "        # missions=['S2', 'L8'],\n",
    "        # filter: ee.Filter.dayOfYear(7*30, 9*30), # summer-only\n",
    "        filter_masked=False, \n",
    "        # filterMaskedFraction = 0.5,\n",
    "        # skip_scene_boundary_fix=False,\n",
    "        # skip_neighborhood_search=False,\n",
    "        neighborhood_search_parameters={\"erosion\": 0, \"dilation\": 0, \"weight\": 50},\n",
    "        bounds_buffer=0,\n",
    "        water_index_min=-0.05,\n",
    "        water_index_max=0.15,\n",
    "        # lowerCdfBoundary=45,\n",
    "        # upperCdfBoundary=50,\n",
    "        # cloud_frequency_threshold_data=0.15, \n",
    "        clip = True\n",
    "    )# .reproject(ee.Projection(\"EPSG:3857\").atScale(90))\n",
    "\n",
    "    image = image.set(\n",
    "        \"fullname\", img_fullname,\n",
    "        \"system:time_start\", ee.Date(start).millis(),\n",
    "        \"system:time_stop\", ee.Date(stop).millis(),\n",
    "        \"zoom\", zoom,\n",
    "        \"tx\", tx,\n",
    "        \"ty\", ty\n",
    "    )\n",
    "\n",
    "    return image\n",
    "\n",
    "def tile_to_asset(\n",
    "    image: ee.Image,\n",
    "    tile: ee.Feature,\n",
    "    export_scale: int,\n",
    "    asset_path_prefix: str,\n",
    "    asset_name: str,\n",
    "    overwrite: bool\n",
    ") -> Optional[ee.batch.Task]:\n",
    "    \n",
    "    asset_id: str = f\"{asset_path_prefix}/{asset_name}\"\n",
    "    asset: Dict[str, Any] = ee.data.getInfo(asset_id)\n",
    "    if overwrite and asset:\n",
    "        logger.info(f\"deleting asset {asset}\")\n",
    "        ee.data.deleteAsset(asset_id)\n",
    "    elif asset:\n",
    "        logger.info(f\"asset {asset} already exists, skipping {asset_name}\")\n",
    "        return\n",
    "    task: ee.batch.Task = ee.batch.Export.image.toAsset(\n",
    "        image,\n",
    "        assetId=asset_id,\n",
    "        description=asset_name,\n",
    "        region=tile.geometry(),\n",
    "        scale=export_scale,\n",
    "        maxPixels= 1e10\n",
    "    )\n",
    "    task.start()\n",
    "    logger.info(f\"exporting {asset_name} to {asset_id}\")\n",
    "\n",
    "def tile_to_cloud_storage(\n",
    "    image: ee.Image,\n",
    "    tile: ee.Feature,\n",
    "    export_scale: int,\n",
    "    bucket: str,\n",
    "    bucket_path: str,\n",
    "    overwrite: bool\n",
    ") -> Optional[ee.batch.Task]:\n",
    "    with build('storage', 'v1') as storage:\n",
    "        res = storage.objects().list(bucket=bucket, prefix=\"/\".join(bucket_path.split(\"/\")[:-1])).execute()\n",
    "    if not overwrite:\n",
    "        try:\n",
    "            object_exists = any(map(lambda item: item.get(\"name\").startswith(bucket_path), res.get(\"items\")))\n",
    "        except AttributeError:\n",
    "            object_exists = False\n",
    "        if object_exists:\n",
    "            logger.info(f\"object {bucket_path} already exists in bucket {bucket}, skipping\")\n",
    "            return\n",
    "        \n",
    "    task: ee.batch.Task = ee.batch.Export.image.toCloudStorage(\n",
    "        image,\n",
    "        bucket=bucket,\n",
    "        description=bucket_path.replace(\"/\", \"_\"),\n",
    "        fileNamePrefix=bucket_path,\n",
    "        region=tile.geometry(),\n",
    "        scale=export_scale,\n",
    "        fileFormat='GeoTIFF',\n",
    "        formatOptions= {'cloudOptimized': True}, # enables easy QGIS plotting\n",
    "        maxPixels= 1e10\n",
    "    )\n",
    "    task.start()\n",
    "    return task\n",
    "\n",
    "def export_sdb_tiles(\n",
    "    sink: str,\n",
    "    tile_list: ee.List,\n",
    "    num_tiles: int,\n",
    "    export_scale: int,\n",
    "    sdb_tiles: ee.ImageCollection,\n",
    "    name_suffix: str,\n",
    "    mode: str,\n",
    "    task_list: List[ee.batch.Task],\n",
    "    overwrite: bool,\n",
    "    bucket: Optional[str] = None\n",
    ") -> List[ee.batch.Task]:\n",
    "    \"\"\"\n",
    "    Export list of tiled images containing sub or intertidal tidal bathymetry. Fires off the tasks and adds to the list of tasks.\n",
    "    based on: https://github.com/gee-community/gee_tools/blob/master/geetools/batch/imagecollection.py#L166\n",
    "\n",
    "    args:\n",
    "        sink (str): type of data sink to export to. Viable options are: \"asset\" and \"cloud\".\n",
    "        tile_list (ee.List): list of tile features.\n",
    "        num_tiles (int): number of tiles in `tile_list`.\n",
    "        scale (int): scale of the export product.\n",
    "        sdb_tiles (ee.ImageCollection): collection of subtidal bathymetry images corresponding\n",
    "            to input tiles.\n",
    "        name_suffix (str): unique identifier after tile statistics.\n",
    "        task_list (List[ee.batch.Task]): list of tasks, adds tasks created to this list.\n",
    "        overwrite (bool): whether to overwrite the current assets under the same `asset_path`.\n",
    "        bucket (str): Bucket where the data is stored. Only used when sink = \"cloud\"\n",
    "    \n",
    "    returns:\n",
    "        List[ee.batch.Task]: list of started tasks\n",
    "\n",
    "    \"\"\"\n",
    "    if sink == \"asset\":\n",
    "        user_name: str = ee.data.getAssetRoots()[0][\"id\"].split(\"/\")[-1]\n",
    "        asset_path_prefix: str = f\"users/{user_name}/eo-bathymetry\"\n",
    "        ee.data.create_assets(asset_ids=[asset_path_prefix], asset_type=\"Folder\", mk_parents=True)\n",
    "    \n",
    "    for i in range(num_tiles):\n",
    "        # get tile\n",
    "        temp_tile: ee.Feature = ee.Feature(tile_list.get(i))\n",
    "        tile_metadata: Dict[str, Any] = temp_tile.getInfo()[\"properties\"]\n",
    "        tx: str = tile_metadata[\"tx\"]\n",
    "        ty: str = tile_metadata[\"ty\"]\n",
    "        zoom: str = tile_metadata[\"zoom\"]\n",
    "        # filter imagecollection based on tile\n",
    "        filtered_ic: ee.ImageCollection = sdb_tiles \\\n",
    "            .filterMetadata(\"tx\", \"equals\", tx) \\\n",
    "            .filterMetadata(\"ty\", \"equals\", ty) \\\n",
    "            .filterMetadata(\"zoom\", \"equals\", zoom)\n",
    "        # if filtered correctly, only a single image remains\n",
    "        img: ee.Image = ee.Image(filtered_ic.first())  # have to cast here\n",
    "        img_name: str = sub(r\"\\.\\d+\", \"\", f\"{mode}/z{zoom}/x{tx}/y{ty}/\") + name_suffix \n",
    "        # Export image\n",
    "        if sink == \"asset\":  # Replace with case / switch in python 3.10\n",
    "            task: Optional[ee.batch.Task] = tile_to_asset(\n",
    "                image=img,\n",
    "                tile=temp_tile,\n",
    "                export_scale=export_scale,\n",
    "                asset_path_prefix=asset_path_prefix,\n",
    "                asset_name=img_name.replace(\"/\",\"_\"),\n",
    "                overwrite=overwrite\n",
    "            )\n",
    "            if task: task_list.append(task)\n",
    "        elif sink == \"cloud\":\n",
    "            if not bucket:\n",
    "                raise ArgumentError(\"Sink option requires \\\"bucket\\\" arg.\")\n",
    "            task: ee.batch.Task = tile_to_cloud_storage(\n",
    "                image=img,\n",
    "                tile=temp_tile,\n",
    "                export_scale=export_scale,\n",
    "                bucket=bucket,\n",
    "                bucket_path=img_name,\n",
    "                overwrite=overwrite\n",
    "            )\n",
    "        else:\n",
    "            raise ArgumentError(\"unrecognized data sink: {sink}\")\n",
    "        task_list.append(task)\n",
    "    return task_list\n",
    "\n",
    "def export_tiles(\n",
    "    sink: str,\n",
    "    mode: str,\n",
    "    geometry: ee.Geometry,\n",
    "    zoom: int,\n",
    "    start: str,\n",
    "    stop: str,\n",
    "    scale: Optional[float] = None,\n",
    "    step_months: int = 3,\n",
    "    window_months: int = 24,\n",
    "    overwrite: bool = False,\n",
    "    bucket: Optional[str] = None\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    From a geometry, creates tiles of input zoom level, calculates subtidal bathymetry in those\n",
    "    tiles, and exports those tiles.\n",
    "\n",
    "    args:\n",
    "        sink (str): type of data sink to export to. Viable options are: \"asset\" and \"cloud\".\n",
    "        mode (str): either \"subtidal\" or \"intertidal\" for select type of bathymetry to export.\n",
    "        geometry (ee.Geometry): geometry of the area of interest.\n",
    "        zoom (int): zoom level of the to-be-exported tiles.\n",
    "        start (ee.String): start date in YYYY-MM-dd format.\n",
    "        stop (ee.String): stop date in YYYY-MM-dd format.\n",
    "        scale Optional(float): scale of the product to be exported. Defaults tiler.zoom_to_scale(zoom).getInfo().\n",
    "        step_months (int): steps with which to roll the window over which the subtidal bathymetry\n",
    "            is calculated.\n",
    "        windows_months (int): number of months over which the bathymetry is calculated.\n",
    "    \"\"\"\n",
    "\n",
    "    def create_year_window(year: ee.Number, month: ee.Number) -> ee.Dictionary:\n",
    "        t: ee.Date = ee.Date.fromYMD(year, month, 1)\n",
    "        d_format: str = \"YYYY-MM-dd\"\n",
    "        return ee.Dictionary({\n",
    "            \"start\": t.format(d_format),\n",
    "            \"stop\": t.advance(window_months, 'month').format(d_format)\n",
    "            })\n",
    "    \n",
    "    window_length: int = (parse(stop).year-parse(start).year)*12+(parse(stop).month-parse(start).month)\n",
    "    dates: ee.List = ee.List.sequence(parse(start).year, parse(stop).year-window_months/12).map(\n",
    "        lambda year: ee.List.sequence(1, None, step_months, int((window_length-window_months)/step_months)+1).map(partial(create_year_window, year))\n",
    "    ).flatten()\n",
    "    \n",
    "    # Get tiles\n",
    "    tiles: ee.FeatureCollection = tiler.get_tiles_for_geometry(geometry, ee.Number(zoom))\n",
    "\n",
    "    if scale == None: scale: float = tiler.zoom_to_scale(zoom).getInfo() # not specified, defaults to pre-set float\n",
    "    else: scale: scale # specified\n",
    "    task_list: List[ee.batch.Task] = []\n",
    "    num_tiles: int = tiles.size().getInfo()\n",
    "    tile_list: ee.List = tiles.toList(num_tiles)\n",
    "\n",
    "    for date in dates.getInfo():\n",
    "        if mode == \"subtidal\":\n",
    "            sdb_tiles: ee.ImageCollection = tiles.map(\n",
    "                lambda tile: get_tile_subtidal_bathymetry(\n",
    "                    tile=tile,\n",
    "                    start=ee.String(date[\"start\"]),\n",
    "                    stop=ee.String(date[\"stop\"])\n",
    "                ).clip(geometry) # clip individual tiles to match geometry of aoi\n",
    "            )\n",
    "        elif mode == \"intertidal\":\n",
    "            sdb_tiles: ee.ImageCollection = tiles.map(\n",
    "                lambda tile: get_tile_intertidal_bathymetry(\n",
    "                    tile=tile,\n",
    "                    start=ee.String(date[\"start\"]),\n",
    "                    stop=ee.String(date[\"stop\"])\n",
    "                ).clip(geometry).select('ndwi').rename('water_score') # clip individual tiles to match geometry of aoi, select ndwi and rename\n",
    "            )\n",
    "\n",
    "        # Now export tiles\n",
    "        export_sdb_tiles(\n",
    "            sink=sink,\n",
    "            tile_list=tile_list,\n",
    "            num_tiles=num_tiles,\n",
    "            mode=mode,\n",
    "            export_scale=scale,\n",
    "            sdb_tiles=sdb_tiles,\n",
    "            name_suffix=f\"t{date['start']}_{date['stop']}\",\n",
    "            task_list=task_list,\n",
    "            overwrite=overwrite,\n",
    "            bucket=bucket\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code for arbitrary polygon run, start to incoporate into defs \n",
    "# TODO: continue def integration of code below for arbitrary polgyon\n",
    "# TODO: look at scale def in sdb.compute... that is dependent on zoom level (how to make this work)\n",
    "def get_subtidal_bathymetry(geom: ee.Feature, start: ee.String, stop: ee.String) -> ee.Image:\n",
    "    \"\"\"\n",
    "    Get subtidal bathymetry based on arbitrary geometry.\n",
    "    Server-side compliant for GEE.\n",
    "\n",
    "    args:\n",
    "        geom (ee.Feature): geometry used to obtain bathymetry.\n",
    "        start (ee.String): start date in YYYY-MM-dd format.\n",
    "        stop (ee.String): stop date in YYYY-MM-dd format.\n",
    "    \n",
    "    returns:\n",
    "        ee.Image: image containing subtidal bathymetry covering geometry.\n",
    "    \"\"\"\n",
    "\n",
    "    bounds: ee.Geometry = ee.Feature(geom).geometry().bounds(1)\n",
    "    sdb: Bathymetry = Bathymetry()\n",
    "    tile_name: ee.String = ee.String(project_name)\n",
    "    img_fullname: ee.String = ee.String(tile_name).cat(\"_t\").cat(ee.Date(start).millis().format())\n",
    "        \n",
    "    image: ee.Image = sdb.compute_inverse_depth(\n",
    "                bounds=bounds,\n",
    "                start=start,\n",
    "                stop=stop,\n",
    "                scale=tiler.zoom_to_scale(ee.Number.parse(\"10\")).multiply(5), # scale to search for clean images\n",
    "                missions=[\"S2\", \"L8\"],\n",
    "                filter_masked=True,\n",
    "                skip_neighborhood_search=False\n",
    "                # cloud_frequency_threshold_data=,\n",
    "                # pansharpen=,\n",
    "                # skip_scene_boundary_fix=,\n",
    "                # bounds_buffer=\n",
    "    ).clip(bounds)\n",
    "\n",
    "    image = image.set(\n",
    "        \"fullname\", img_fullname,\n",
    "        \"system:time_start\", ee.Date(start).millis(),\n",
    "        \"system:time_stop\", ee.Date(stop).millis(),\n",
    "    )\n",
    "    return image\n",
    "\n",
    "check = get_subtidal_bathymetry(geom=ee.Feature(bounds), start=start_date, stop=stop_date)\n",
    "\n",
    "def create_year_window(year: ee.Number, month: ee.Number) -> ee.Dictionary:\n",
    "    t: ee.Date = ee.Date.fromYMD(year, month, 1)\n",
    "    d_format: str = \"YYYY-MM-dd\"\n",
    "    return ee.Dictionary({\n",
    "        \"start\": t.format(d_format),\n",
    "        \"stop\": t.advance(compo_len, 'month').format(d_format)\n",
    "        })\n",
    "\n",
    "window_length: int = (parse(stop_date).year-parse(start_date).year)*12+(parse(stop_date).month-parse(start_date).month)\n",
    "dates: ee.List = ee.List.sequence(parse(start_date).year, parse(stop_date).year-compo_len/12).map(\n",
    "    lambda year: ee.List.sequence(1, None, compo_int, int((window_length-compo_len)/compo_int)+1).map(partial(create_year_window, year))\n",
    ").flatten()\n",
    "\n",
    "for date in dates.getInfo():\n",
    "\n",
    "    bucket_path = sub(r\"\\.\\d+\", \"\", f\"{mode}/\") + f\"t{date['start']}_{date['stop']}\"\n",
    "\n",
    "    task = ee.batch.Export.image.toCloudStorage(\n",
    "        check.clip(bounds),\n",
    "        bucket=bucket,\n",
    "        description=bucket_path.replace(\"/\", \"_\"),\n",
    "        fileNamePrefix=bucket_path,\n",
    "        region=ee.Feature(bounds).geometry(),\n",
    "        scale=scale,\n",
    "        fileFormat='GeoTIFF',\n",
    "        formatOptions= {'cloudOptimized': True}, # enables easy QGIS plotting\n",
    "        maxPixels= 1e10\n",
    "    )\n",
    "    task.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# similar code for arbitrary polygon run, yet only plain functions for subtidal (no defs), very simplified\n",
    "sdb = Bathymetry() # initialize sdb instance (class)\n",
    "\n",
    "image = sdb.compute_inverse_depth(\n",
    "    bounds=bounds,\n",
    "    start=start_date,\n",
    "    stop=stop_date,\n",
    "    scale=tiler.zoom_to_scale(ee.Number.parse(\"10\")).multiply(5),\n",
    "    missions=['S2', 'L8'],\n",
    "    filter_masked=True,\n",
    "    skip_neighborhood_search=False,\n",
    ").clip(bounds) # clip to bounds\n",
    "\n",
    "bucket_path = sub(r\"\\.\\d+\", \"\", f\"{mode}/\") + f\"t{start_date}_{stop_date}\"\n",
    "\n",
    "task = ee.batch.Export.image.toCloudStorage(\n",
    "    image=image.clip(bounds),\n",
    "    bucket=bucket,\n",
    "    description=bucket_path.replace(\"/\", \"_\"),\n",
    "    fileNamePrefix=bucket_path,\n",
    "    region=bounds,\n",
    "    scale=scale,\n",
    "    fileFormat='GeoTIFF',\n",
    "    formatOptions= {'cloudOptimized': True}, # enables easy QGIS plotting\n",
    "    maxPixels= 1e10\n",
    ")\n",
    "task.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# similar code for arbitrary polygon run, yet only plain functions for intertidal (no defs), very simplified\n",
    "mode = 'intertidal'\n",
    "image = sdb.compute_intertidal_depth(\n",
    "    bounds=bounds,\n",
    "    start=start_date,\n",
    "    stop=stop_date,\n",
    "    scale=tiler.zoom_to_scale(ee.Number.parse(\"10\")).multiply(5),\n",
    "    # missions=['S2', 'L8'],\n",
    "    # filter: ee.Filter.dayOfYear(7*30, 9*30), # summer-only\n",
    "    filter_masked=False, \n",
    "    # filterMaskedFraction: 0.5,\n",
    "    # skip_scene_boundary_fix=False,\n",
    "    # skip_neighborhood_search=False,\n",
    "    neighborhood_search_parameters={\"erosion\": 0, \"dilation\": 0, \"weight\": 50},\n",
    "    bounds_buffer=0,\n",
    "    water_index_min=-0.05,\n",
    "    water_index_max=0.15,\n",
    "    # lowerCdfBoundary: 45,\n",
    "    # upperCdfBoundary: 50\n",
    "    # cloud_frequency_threshold_data=0.15,\n",
    "    clip = True\n",
    ")# .reproject(ee.Projection(\"EPSG:3857\").atScale(90))\n",
    "\n",
    "bucket_path = sub(r\"\\.\\d+\", \"\", f\"{mode}/\") + f\"t{start_date}_{stop_date}\"\n",
    "\n",
    "task = ee.batch.Export.image.toCloudStorage(\n",
    "    image=image.clip(bounds).select('ndwi').rename('water_score'),\n",
    "    bucket=bucket,\n",
    "    description=bucket_path.replace(\"/\", \"_\"),\n",
    "    fileNamePrefix=bucket_path,\n",
    "    region=bounds,\n",
    "    scale=scale,\n",
    "    fileFormat='GeoTIFF',\n",
    "    formatOptions= {'cloudOptimized': True}, # enables easy QGIS plotting\n",
    "    maxPixels= 1e10\n",
    ")\n",
    "task.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: do intertidal mapping on single NDWI images as stated by Robyn & Maarten\n",
    "# compute bathy and export to GCS\n",
    "# when run submitted, check task progress at: https://code.earthengine.google.com/tasks\n",
    "\n",
    "# export bathymetry based on standardized tiling practice\n",
    "export_tiles(sink=\"cloud\", mode=mode, geometry=bounds, zoom=zoomed_list[sel_tile], start=start_date, stop=stop_date, \n",
    "             scale=scale, step_months=compo_int, window_months=compo_len, overwrite=True, bucket=bucket)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored:  z11_x1331_y880_t2020-01-01_2022-01-01.tif\n",
      "Stored:  z11_x1331_y880_t2021-01-01_2022-01-01.tif\n",
      "Stored:  z11_x1331_y881_t2020-01-01_2022-01-01.tif\n",
      "Stored:  z11_x1331_y881_t2021-01-01_2022-01-01.tif\n",
      "Stored:  z11_x1332_y879_t2020-01-01_2022-01-01.tif\n",
      "Stored:  z11_x1332_y879_t2021-01-01_2022-01-01.tif\n",
      "Stored:  z11_x1332_y880_t2020-01-01_2022-01-01.tif\n",
      "Stored:  z11_x1332_y880_t2021-01-01_2022-01-01.tif\n",
      "Stored:  z11_x1332_y881_t2020-01-01_2022-01-01.tif\n",
      "Stored:  z11_x1332_y881_t2021-01-01_2022-01-01.tif\n",
      "Stored:  z11_x1332_y882_t2020-01-01_2022-01-01.tif\n",
      "Stored:  z11_x1332_y882_t2021-01-01_2022-01-01.tif\n",
      "Stored:  z11_x1333_y879_t2020-01-01_2022-01-01.tif\n",
      "Stored:  z11_x1333_y879_t2021-01-01_2022-01-01.tif\n",
      "Stored:  z11_x1333_y880_t2020-01-01_2022-01-01.tif\n",
      "Stored:  z11_x1333_y880_t2021-01-01_2022-01-01.tif\n",
      "Stored:  z11_x1333_y881_t2020-01-01_2022-01-01.tif\n",
      "Stored:  z11_x1333_y881_t2021-01-01_2022-01-01.tif\n",
      "Stored:  z11_x1333_y882_t2020-01-01_2022-01-01.tif\n",
      "Stored:  z11_x1333_y882_t2021-01-01_2022-01-01.tif\n",
      "Stored:  z11_x1334_y880_t2020-01-01_2022-01-01.tif\n",
      "Stored:  z11_x1334_y880_t2021-01-01_2022-01-01.tif\n",
      "Stored:  z11_x1334_y881_t2020-01-01_2022-01-01.tif\n",
      "Stored:  z11_x1334_y881_t2021-01-01_2022-01-01.tif\n",
      "Stored:  z9_x332_y220_t2021-01-01_2022-01-01.tif\n",
      "Stored:  z9_x333_y219_t2021-01-01_2022-01-01.tif\n",
      "Stored:  z9_x333_y220_t2021-01-01_2022-01-01.tif\n"
     ]
    }
   ],
   "source": [
    "# store locally (from GCS) to visualize in QGIS / ArcGIS (can also download manually via Cloud Storage platform)\n",
    "\n",
    "# create or check if local storage folder is present\n",
    "if not os.path.exists(os.path.join(main_fol, output_fol)):\n",
    "    os.makedirs(os.path.join(main_fol, output_fol))\n",
    "\n",
    "# get file names\n",
    "client = storage.Client()\n",
    "ls = [blob for blob in client.list_blobs(bucket)] \n",
    "\n",
    "# downloading composites to a local folder while only keeping subtidal / intertidal folder (i.e. other nested folders are flattened)\n",
    "check_files = []\n",
    "for blob in ls:\n",
    "    #if \"tif\" in blob.name:\n",
    "    mode_fol = blob.name.split('/')[0]\n",
    "    if mode_fol == mode:\n",
    "        file_name = \"_\".join(blob.name.split('/')[1:])\n",
    "        check_files.append(file_name) #blob.name.split('/')[-1]\n",
    "        if not os.path.exists(os.path.join(main_fol, output_fol, mode_fol)): # create subfolder\n",
    "            os.makedirs(os.path.join(main_fol, output_fol, mode_fol))\n",
    "        blob.download_to_filename(os.path.join(main_fol, output_fol, mode_fol, file_name))\n",
    "        print('Stored: ', file_name) # check progress\n",
    "\n",
    "# elaborate on possibility of storing locally\n",
    "if len(check_files) == 0:\n",
    "    print('Please enable GCS storeing of images first, before toggling on local storage option')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "geo_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "92590b8b61d447ec77a1c897af89012828c879bd382a813b35f1d19994b1169c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
