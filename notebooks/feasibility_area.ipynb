{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nThis code generates a fesibility area for deriving intertidal bathymetry. \\nIt computes the area between the the Lowest Astronomical Tide (LAT) and \\nthe Mean Seal Level + 10 m, based on a LAT map, and a Bathymetry and \\nelevation GEBCO map. \\n\\n    Author: Mario.FuentesMonjaraz@deltares.nl\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "    \"\"\"\n",
    "    This code generates a fesibility area for deriving intertidal bathymetry. \n",
    "    It computes the area between the the Lowest Astronomical Tide (LAT) and \n",
    "    the Mean Seal Level + 10 m, based on a LAT map, and a Bathymetry and \n",
    "    elevation GEBCO map. \n",
    "\n",
    "        Author: Mario.FuentesMonjaraz@deltares.nl\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import dask\n",
    "import xarray as xr\n",
    "import pyproj\n",
    "import rioxarray\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign_projection(ds, epsg=None):\n",
    "\n",
    "    if not epsg == None:\n",
    "        proj = pyproj.CRS.from_epsg(int(epsg))\n",
    "    else:\n",
    "        proj = pyproj.CRS.from_epsg(int(ds.crs.values.tolist()))\n",
    "\n",
    "    print(proj,\"projection was assigned to the dataset attributes\")\n",
    "    ds.attrs['crs'] = proj\n",
    "    return ds\n",
    "\n",
    "def print_ds_properties(rds,epsg=None):\n",
    "    # Print the grid size\n",
    "    print(\"Grid size:\", rds.rio.resolution())\n",
    "\n",
    "    #Print null data\n",
    "    print(\"no data:\", rds.rio.nodata)\n",
    "\n",
    "    # Print the projection information\n",
    "    if rds.rio.crs == None:\n",
    "        print(\"There is no projection\")\n",
    "        proj = pyproj.CRS.from_epsg(epsg)\n",
    "        rds.attrs['crs'] = proj\n",
    "        rds.rio.set_crs(proj, inplace=True)\n",
    "    else:\n",
    "        print(\"There is projection available\")\n",
    "    \n",
    "    print(\"Projection EPSG code is:\", rds.rio.crs, \"\\n\")\n",
    "    return\n",
    "\n",
    "def change_resolution(ds, new_resolution):\n",
    "    # Reproject the rioxarray object to the new resolution\n",
    "    reprojected_ds = ds.rio.reproject(ds.rio.crs, resolution=new_resolution, resampling=\"bilinear\")\n",
    "    return reprojected_ds\n",
    "\n",
    "def match_resolution(rds, rds_source):\n",
    "    # Reproject the rioxarray object to the new resolution\n",
    "    reprojected_ds = rds.rio.reproject(rds_source.rio.crs, resolution=rds_source.rio.resolution(), resampling= rioxarray.enums.Resampling.bilinear)\n",
    "    return reprojected_ds\n",
    "\n",
    "def redefine_null_for_nan(ds, new_null_value):\n",
    "    # Replace NaN values with the new null value\n",
    "    ds.values[np.isnan(ds.values)] = new_null_value\n",
    "    # ds.rio.update({'nodata': new_null_value})\n",
    "    return ds\n",
    "\n",
    "def create_gdf_from_geojson_files(input_aoi_data):\n",
    "    geojson_files = []\n",
    "\n",
    "    for filename in os.listdir(input_aoi_data):\n",
    "        if filename.endswith(\".geojson\"):\n",
    "            gdf = gpd.read_file(os.path.join(input_aoi_data,filename))\n",
    "            aoi_id = filename.split('_')\n",
    "            aoi_id = aoi_id[-1].split('.')[0]\n",
    "            gdf.insert(1, \"aoi\", aoi_id)\n",
    "            geojson_files.append((gdf))\n",
    "\n",
    "    aoi_gdf = gpd.GeoDataFrame(pd.concat(geojson_files, ignore_index=True)).drop(columns=[\"id\"])\n",
    "    return aoi_gdf\n",
    "\n",
    "def clip_raster(rds, geometry):\n",
    "    rds_clipped = rds.rio.clip(geometry)\n",
    "    return rds_clipped\n",
    "\n",
    "def clip_raster_with_gdf(rds, gdf):\n",
    "    rds_clipped_list = []\n",
    "    for index, row in gdf.iterrows():\n",
    "        aoi = row[\"aoi\"]\n",
    "        try:\n",
    "            geometry = gdf.iloc[index:index+1].geometry\n",
    "            rds_clipped = rds.rio.clip(geometry)\n",
    "            rds_clipped_list.append(rds_clipped)\n",
    "            print(f\"Successful processing row {index} {aoi}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing row {index} {aoi}: {e}\")\n",
    "            rds_clipped_list.append(\"NaN\")\n",
    "            continue\n",
    "    print(\"\\n\")\n",
    "    return rds_clipped_list \n",
    "\n",
    "\n",
    "def apply_lat_mask(raster1, raster2):\n",
    "    # Read the raster data\n",
    "    data1 = raster1.values\n",
    "    data2 = raster2.values\n",
    "    \n",
    "    # Create a new array for the result raster\n",
    "    result_data = np.full_like(data1, fill_value=np.nan, dtype='float32')\n",
    "    \n",
    "    # Compare pixel values and assign new values\n",
    "    result_data[(data1 >= data2)] = 7\n",
    "    result_data[(data1 < data2)] = 9\n",
    "    \n",
    "    # Create a new rioxarray dataset for the result raster\n",
    "    result_raster = raster1.copy(data=result_data)\n",
    "    \n",
    "    return result_raster\n",
    "\n",
    "\n",
    "def apply_gebco_mask(raster1, raster2):\n",
    "    # Read the raster data\n",
    "    data1 = raster1.values\n",
    "    data2 = raster2.values\n",
    "    \n",
    "    # Create a new array for the result raster\n",
    "    result_data = np.full_like(data1, fill_value=np.nan, dtype='float32')\n",
    "    \n",
    "    # Compare pixel values and assign new values\n",
    "    result_data[(data2 > 10)] = 11\n",
    "    \n",
    "    # Create a new rioxarray dataset for the result raster\n",
    "    result_raster = raster1.copy(data=result_data)\n",
    "    \n",
    "    return result_raster\n",
    "\n",
    "def apply_gebco_mask(gebco_rds,lat_mask):\n",
    "\n",
    "    binary_mask = gebco_rds > 10\n",
    "    lat_mask_gebco_plus_10 = lat_mask.where(~binary_mask, other=11)\n",
    "\n",
    "    return lat_mask_gebco_plus_10 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define packages paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data path already exists: d:\\Projects\\Copernicus\\Ouput\n"
     ]
    }
   ],
   "source": [
    "repository_path = os.path.dirname(os.getcwd())\n",
    "input_data_path = os.path.join(repository_path,\"Data\")\n",
    "input_aoi_data = r\"p:\\11209821-cmems-global-sdb\\00_miscellaneous\\AOIs\"\n",
    "output_data_path = os.path.join(repository_path,\"Ouput\")\n",
    "\n",
    "if not os.path.exists(output_data_path):\n",
    "    print(\"Output data path does not exist. Creating directory...\")\n",
    "    os.makedirs(output_data_path)\n",
    "    print(\"Output data path created:\", output_data_path)\n",
    "else:\n",
    "    print(\"Input data path already exists:\", output_data_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieve input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\fuentesm\\AppData\\Local\\anaconda3\\envs\\copernicus\\Lib\\site-packages\\xarray\\core\\dataset.py:271: UserWarning: The specified chunks separate the stored chunks along dimension \"lat\" starting at index 100. This could degrade performance. Instead, consider rechunking after loading.\n",
      "  warnings.warn(\n",
      "c:\\Users\\fuentesm\\AppData\\Local\\anaconda3\\envs\\copernicus\\Lib\\site-packages\\xarray\\core\\dataset.py:271: UserWarning: The specified chunks separate the stored chunks along dimension \"lon\" starting at index 100. This could degrade performance. Instead, consider rechunking after loading.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# gebco_ds = xr.open_dataset(os.path.join(input_data_path,'gebco_2023_n90.0_s0.0_w0.0_e90.0_depthmsl.nc')) \n",
    "depthmsl_ds = xr.open_dataset(os.path.join(input_data_path,'gebco_2023_n90.0_s0.0_w0.0_e90.0_depthmsl.nc'), chunks={'lat': 100, 'lon':100})\n",
    "depthmsl_rds = rds = rioxarray.open_rasterio(os.path.join(input_data_path,'gebco_2023_n90.0_s0.0_w0.0_e90.0_depthmsl.nc'), chunks={'y': 100, 'x':100})\n",
    "\n",
    "lat_ds = xr.open_dataset(os.path.join(input_data_path,'gebco_2023_n90.0_s0.0_w0.0_e90.0_lat.nc'), chunks={'lat': 100, 'lon':100})\n",
    "lat_rds = rioxarray.open_rasterio(os.path.join(input_data_path,'gebco_2023_n90.0_s0.0_w0.0_e90.0_lat.nc'), chunks={'y': 100, 'x':100})\n",
    "\n",
    "lw_ds = xr.open_dataset(os.path.join(input_data_path,'LandWater15ARC.nc'), chunks={'lat': 100, 'lon':100})\n",
    "lw_rds = rioxarray.open_rasterio(os.path.join(input_data_path,'LandWater15ARC.nc'), chunks={'y': 100, 'x':100})\n",
    "\n",
    "gebco_ds = xr.open_dataset(os.path.join(input_data_path,'gebco_2023_n90.0_s0.0_w0.0_e90.0.tif'), chunks={'lat': 100, 'lon':100})\n",
    "gebco_rds = rioxarray.open_rasterio(os.path.join(input_data_path,'gebco_2023_n90.0_s0.0_w0.0_e90.0.tif'), chunks={'y': 100, 'x':100})\n",
    "\n",
    "# AOIs for al the sites\n",
    "aoi_gdf = create_gdf_from_geojson_files(input_aoi_data)\n",
    "\n",
    "# AOI for Sao Paulo\n",
    "aoi_saopaulo = aoi_gdf[aoi_gdf['aoi'] == \"SaoPaulo\"]\n",
    "aoi_saopaulo = aoi_gdf.iloc[0:1]\n",
    "aoi_saopaulo.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# AOI for Okha\n",
    "aoi_wadden_okha = aoi_gdf[aoi_gdf['aoi'] == \"Okha\"]\n",
    "aoi_wadden_okha = aoi_gdf.iloc[3:5]\n",
    "aoi_wadden_okha.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Bounding boxes of the areas of interes\n",
    "aoi_gdf_bounding_box  = aoi_gdf.copy()\n",
    "aoi_gdf_bounding_box['bounding_box'] = aoi_gdf_bounding_box.geometry.apply(lambda x: x.envelope)\n",
    "aoi_gdf_bounding_box = aoi_gdf_bounding_box[['aoi', 'bounding_box']].rename(columns={'bounding_box': 'geometry'})\n",
    "\n",
    "# AOI for Sao Paulo\n",
    "aoi_saopaulo_bounding_box = aoi_gdf_bounding_box[aoi_gdf_bounding_box['aoi'] == \"SaoPaulo\"]\n",
    "aoi_saopaulo_bounding_box = aoi_gdf_bounding_box.iloc[0:1]\n",
    "aoi_saopaulo_bounding_box.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# AOI for Okha\n",
    "aoi_wadden_okha_bounding_box = aoi_gdf_bounding_box[aoi_gdf_bounding_box['aoi'] == \"Okha\"]\n",
    "aoi_wadden_okha_bounding_box = aoi_gdf_bounding_box.iloc[3:5]\n",
    "aoi_wadden_okha_bounding_box.reset_index(drop=True, inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Harmonize input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPSG:4326 projection was assigned to the dataset attributes\n",
      "EPSG:4326 projection was assigned to the dataset attributes\n",
      "EPSG:4326 projection was assigned to the dataset attributes\n",
      "EPSG:4326 projection was assigned to the dataset attributes\n",
      "Grid size: (0.004166666666666666, -0.004166666666666667)\n",
      "no data: 9999.0\n",
      "There is projection available\n",
      "Projection EPSG code is: EPSG:4326 \n",
      "\n",
      "Grid size: (0.004166666666666666, -0.004166666666666667)\n",
      "no data: 9999.0\n",
      "There is projection available\n",
      "Projection EPSG code is: EPSG:4326 \n",
      "\n",
      "Grid size: (0.004166666501832161, -0.004166666690214726)\n",
      "no data: 255\n",
      "There is no projection\n",
      "Projection EPSG code is: EPSG:4326 \n",
      "\n",
      "Grid size: (0.004166666666666666, -0.004166666666666667)\n",
      "no data: -32767\n",
      "There is projection available\n",
      "Projection EPSG code is: EPSG:4326 \n",
      "\n",
      "Grid size: (0.004166666666666666, -0.004166666666666667)\n",
      "no data: nan\n",
      "There is projection available\n",
      "Projection EPSG code is: EPSG:4326 \n",
      "\n",
      "Grid size: (0.004166666666666666, -0.004166666666666667)\n",
      "no data: nan\n",
      "There is projection available\n",
      "Projection EPSG code is: EPSG:4326 \n",
      "\n",
      "Grid size: (0.004166666666666666, -0.004166666666666667)\n",
      "no data: nan\n",
      "There is projection available\n",
      "Projection EPSG code is: EPSG:4326 \n",
      "\n",
      "Grid size: (0.004166666666666666, -0.004166666666666667)\n",
      "no data: nan\n",
      "There is projection available\n",
      "Projection EPSG code is: EPSG:4326 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "depthmsl_ds = assign_projection(depthmsl_ds)\n",
    "lat_ds = assign_projection(lat_ds)\n",
    "lw_ds = assign_projection(lw_ds, 4326)\n",
    "gebco_ds = assign_projection(gebco_ds, 4326)\n",
    "\n",
    "print_ds_properties(depthmsl_rds)\n",
    "print_ds_properties(lat_rds)\n",
    "print_ds_properties(lw_rds, 4326)\n",
    "print_ds_properties(gebco_rds, 4326)\n",
    "\n",
    "lw_rds.rio.reproject_match(depthmsl_rds)\n",
    "\n",
    "depthmsl_rds = depthmsl_rds.rio.set_nodata(np.nan)\n",
    "lat_rds      = lat_rds.rio.set_nodata(np.nan)\n",
    "lw_rds       = lat_rds.rio.set_nodata(np.nan)\n",
    "gebco_rds    = lat_rds.rio.set_nodata(np.nan)\n",
    "\n",
    "print_ds_properties(depthmsl_rds)\n",
    "print_ds_properties(lat_rds)\n",
    "print_ds_properties(lw_rds, 4326)\n",
    "print_ds_properties(gebco_rds, 4326)\n",
    "\n",
    "depthmsl_rds.rio.to_raster(os.path.join(output_data_path,'depthmsl_rds.tif'), driver='GTiff', compress='lzw')\n",
    "lat_rds.rio.to_raster(os.path.join(output_data_path,'lat_rds.tif'), driver='GTiff', compress='lzw')\n",
    "lw_rds.rio.to_raster(os.path.join(output_data_path,'lw_rds.tif'), driver='GTiff', compress='lzw')\n",
    "gebco_rds.rio.to_raster(os.path.join(output_data_path,'gebco_rds.tif'), driver='GTiff', compress='lzw')\n",
    "\n",
    "# If the harmonized data was already created then they can just loaded\n",
    "# depthmsl_rds = rioxarray.open_rasterio(os.path.join(output_data_path,'depthmsl_rds.tif'))\n",
    "# lat_rds = rioxarray.open_rasterio(os.path.join(output_data_path,'lat_rds.tif'))\n",
    "# lw_rds = rioxarray.open_rasterio(os.path.join(output_data_path,'lw_rds.tif'))\n",
    "# gebco_rds = rioxarray.open_rasterio(os.path.join(output_data_path,'gebco_rds.tif'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Intertidal Zone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "lat_mask = apply_lat_mask(depthmsl_rds, lat_rds)\n",
    "lat_mask_gebco_plus_10 = apply_gebco_mask(gebco_rds,lat_mask)\n",
    "\n",
    "lat_mask.rio.to_raster(os.path.join(output_data_path,\"lat_mask.tif\"))\n",
    "lat_mask_gebco_plus_10.rio.to_raster(os.path.join(output_data_path,\"lat_mask_gebco_plus_10.tif\"))\n",
    "\n",
    "# If the masked data was already created then they can just loaded\n",
    "# lat_mask = rioxarray.open_rasterio(os.path.join(output_data_path,\"lat_mask.tif\"))\n",
    "# lat_mask_gebco_plus_10 = rioxarray.open_rasterio(os.path.join(output_data_path,\"lat_mask_gebco_plus_10.tif\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clip results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successful processing row 0 WaddenSea\n",
      "Successful processing row 1 Okha\n",
      "\n",
      "\n",
      "Successful processing row 0 WaddenSea\n",
      "Successful processing row 1 Okha\n",
      "\n",
      "\n",
      "Successful processing row 0 WaddenSea\n",
      "Successful processing row 1 Okha\n",
      "\n",
      "\n",
      "Successful processing row 0 WaddenSea\n",
      "Successful processing row 1 Okha\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "depthmsl_rds_clipped_wadden_okha = clip_raster_with_gdf(depthmsl_rds, aoi_wadden_okha_bounding_box)\n",
    "lat_rds_clipped_wadden_okha = clip_raster_with_gdf(lat_rds, aoi_wadden_okha_bounding_box)\n",
    "lw_rds_clipped_wadden_okha = clip_raster_with_gdf(lw_rds, aoi_wadden_okha_bounding_box)\n",
    "gebco_rds_clipped_wadden_okha = clip_raster_with_gdf(gebco_rds, aoi_wadden_okha_bounding_box)\n",
    "\n",
    "lat_mask_gebco_plus_10.rio.set_crs(\"epsg:4326\", inplace=True)\n",
    "lat_mask_gebco_plus_10_clipped_wadden_okha = clip_raster_with_gdf(lat_mask_gebco_plus_10, aoi_wadden_okha_bounding_box)\n",
    "\n",
    "lat_rds_clipped_wadden_okha[0].rio.to_raster(os.path.join(output_data_path,'lat_rds_clipped_wadden.tif'), driver='GTiff', compress='lzw')\n",
    "lat_rds_clipped_wadden_okha[1].rio.to_raster(os.path.join(output_data_path,'lat_rds_clipped_okha.tif'), driver='GTiff', compress='lzw')\n",
    "depthmsl_rds_clipped_wadden_okha[0].rio.to_raster(os.path.join(output_data_path,'depthmsl_rds_clipped_wadden.tif'), driver='GTiff', compress='lzw')\n",
    "depthmsl_rds_clipped_wadden_okha[1].rio.to_raster(os.path.join(output_data_path,'depthmsl_rds_clipped_okha.tif'), driver='GTiff', compress='lzw')\n",
    "lw_rds_clipped_wadden_okha[0].rio.to_raster(os.path.join(output_data_path,'lw_rds_clipped_wadden.tif'), driver='GTiff', compress='lzw')\n",
    "lw_rds_clipped_wadden_okha[1].rio.to_raster(os.path.join(output_data_path,'lw_rds_clipped_okha.tif'), driver='GTiff', compress='lzw')\n",
    "gebco_rds_clipped_wadden_okha[0].rio.to_raster(os.path.join(output_data_path,'gebco_rds_clipped_wadden.tif'), driver='GTiff', compress='lzw')\n",
    "gebco_rds_clipped_wadden_okha[1].rio.to_raster(os.path.join(output_data_path,'gebco_rds_clipped_okha.tif'), driver='GTiff', compress='lzw')\n",
    "\n",
    "lat_mask_gebco_plus_10_clipped_wadden_okha[0].rio.to_raster(os.path.join(output_data_path,'lat_mask_gebco_plus_10_clipped_wadden.tif'), driver='GTiff', compress='lzw')\n",
    "lat_mask_gebco_plus_10_clipped_wadden_okha[1].rio.to_raster(os.path.join(output_data_path,'lat_mask_gebco_plus_10_clipped_okha.tif'), driver='GTiff', compress='lzw')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "copernicus",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
